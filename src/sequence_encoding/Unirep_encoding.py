'''This code is responsible for encoding the Unirep data. The Unirep
and fair-esm require input to be formatted a little differently from
the other encodings used in these experiments -- 
also, in contrast to the other encodings, Unirep and fair-esm do not use Chothia
numbering. The sequence encodings are generated by writing to a FASTA
file with the category info etc in the descriptions, then
feeding the fasta to the TAPE project tokenizer with the pretrained unirep
language model.'''

#Author: Jonathan Parkinson <jlparkinson1@gmail.com>

import os, numpy as np, torch, argparse, sys
import subprocess
from ..utilities.model_data_loader import load_data


#This function places two command line subprocess calls to TAPE
#to embed the aa sequences using the unirep pretrained model. These
#calls will generate npz files. This piece is computationally
#more expensive than any of the other encoding operations -- 
#expect this to take a little longer than generating the one-hot
#or autoencoder sequences.
def encode():
    subprocess.call(["tape-embed", "unirep", "fair_unirep_train.faa", "train.npz",
        "babbler-1900", "--tokenizer", "unirep"])
    subprocess.call(["tape-embed", "unirep", "fair_unirep_test.faa", "test.npz",
        "babbler-1900", "--tokenizer", "unirep"])

#This function converts the npz files generated by the TAPE call
#into PyTorch tensors.
def convert_npz_to_torch():
    test = np.load("test.npz", allow_pickle=True)
    train = np.load("train.npz", allow_pickle=True)
    testlist = list(test.keys())
    trainlist = list(train.keys())
    xtrain, xtest, ytrain, ytest = [], [], [], []
    for key in trainlist:
        xtrain.append(train[key].tolist()["pooled"])
        ytrain.append(np.asarray([float(z) for z in key.split('_')[1:]]))
    for key in testlist:
        xtest.append(test[key].tolist()["pooled"])
        ytest.append(np.asarray([float(z) for z in key.split('_')[1:]]))

    xtrain, xtest = np.stack(xtrain), np.stack(xtest)
    ytrain, ytest = np.stack(ytrain), np.stack(ytest)
    xtrain = torch.from_numpy(xtrain).float()
    xtest = torch.from_numpy(xtest).float()
    ytrain = torch.from_numpy(ytrain).float()
    ytest = torch.from_numpy(ytest).float()
    torch.save(xtrain, "unirep_trainx.pt")
    torch.save(xtest, "unirep_testx.pt")
    torch.save(ytrain, "unirep_trainy.pt")
    torch.save(ytest, "unirep_testy.pt")
    os.remove("train.npz")
    os.remove("test.npz")

#Convenience function for generating the unirep encoding.
def unirep_encoding_wrapper(start_dir):
    os.chdir(os.path.join(start_dir, "encoded_data"))
    fnames = os.listdir()
    for fname in ["fair_unirep_train.faa", "fair_unirep_test.faa"]:
        if fname not in fnames:
            print("Fasta files not yet generated.")
            return
    os.chdir(start_dir)
    encoded_data = load_data(start_dir, "unirep")
    if encoded_data[0] is not None:
        print("Unirep already encoded.")
        return
    encode()
    os.chdir(os.path.join(start_dir, "encoded_data"))
    convert_npz_to_torch()

